{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overall Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Diagram\n",
    "2. Explaination of the pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data loading\n",
    "\n",
    "In this section, any dataset can be used. Including dataset that doesn't have any aspect terms or categories, and dataset that doesn't have polarity. For this example, we will used an existing dataset for aspect based sentiment analysis called semeval2014 task 4 which include aspect terms and categories analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset sem_eval2014_task4_raw (C:/Users/User/.cache/huggingface/datasets/Yaxin___sem_eval2014_task4_raw/laptops/0.0.1/3f2b4b42aa35876b7faba99ae6f73b106955b8c9162c6fc5160fd74497f7790f)\n",
      "Found cached dataset sem_eval2014_task4_raw (C:/Users/User/.cache/huggingface/datasets/Yaxin___sem_eval2014_task4_raw/restaurants/0.0.1/3f2b4b42aa35876b7faba99ae6f73b106955b8c9162c6fc5160fd74497f7790f)\n"
     ]
    }
   ],
   "source": [
    "# Load dataset from huggingface\n",
    "from datasets import load_dataset\n",
    "laptop_dataset = load_dataset(\"Yaxin/SemEval2014Task4Raw\", name='laptops', split='train')\n",
    "restaurant_dataset = load_dataset(\"Yaxin/SemEval2014Task4Raw\", name='restaurants', split='train')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The test dataset doesn't have any polarity but it does have terms and categories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next will be preprocessing the data. In this sample, most of the text dataset are pretty cleaned. But for the sake of this demo, we will clean it anyways."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsupervised_absa.preprocess import simple_preprocessing\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing strip_spaces: 100%|██████████| 11/11 [00:00<00:00, 63.90it/s]         \n"
     ]
    }
   ],
   "source": [
    "# preprocessing currently only supports pandas.series, list, and numpy\n",
    "laptop_df = laptop_dataset.to_pandas()\n",
    "laptop_df['text'] = simple_preprocessing(laptop_df['text'])\n",
    "laptop_df = laptop_df.drop_duplicates(subset=['text']).set_index('sentenceId')\n",
    "laptop_preprocessed = Dataset.from_pandas(laptop_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing strip_spaces: 100%|██████████| 11/11 [00:00<00:00, 66.61it/s]         \n"
     ]
    }
   ],
   "source": [
    "# preprocessing currently only supports pandas.series, list, and numpy\n",
    "restaurant_df = restaurant_dataset.to_pandas()\n",
    "restaurant_df['text'] = simple_preprocessing(restaurant_df['text'])\n",
    "restaurant_df = restaurant_df.drop_duplicates(subset=['text']).set_index('sentenceId')\n",
    "restaurant_preprocessed = Dataset.from_pandas(restaurant_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The processes in the preprocessing pipeline include:\n",
    "1. encode decode\n",
    "2. convert_unicode\n",
    "3. remove_url\n",
    "4. remove_control_characters\n",
    "5. remove_tags\n",
    "6. remove_emoji\n",
    "7. convert_contractions\n",
    "8. remove_numbers\n",
    "9. remove_punctuation\n",
    "10. remove_multiple_spaces\n",
    "11. strip_spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4383fa5fde8944bcb9821fdce8668048",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/3028 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac6c76ecbe32475b808bc1c5fb5e2217",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/3030 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "laptop_preprocessed.save_to_disk('../data_2014/preprocessed_data/laptop_dataset')\n",
    "restaurant_preprocessed.save_to_disk('../data_2014/preprocessed_data/restaurant_dataset')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "absa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
