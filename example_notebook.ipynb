{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: unsupervised_absa in /opt/miniconda3/envs/ABSA/lib/python3.10/site-packages (0.1.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install unsupervised_absa"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overall Pipeline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Diagram\n",
    "2. Explaination of the pipeline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset sample"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data loading\n",
    "\n",
    "In this section, any dataset can be used. Including dataset that doesn't have any aspect terms or categories, and dataset that doesn't have polarity. For this example, we will used an existing dataset for aspect based sentiment analysis called semeval2014 task 4 which include aspect terms and categories analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No config specified, defaulting to: sem_eval2014_task4_raw/All\n",
      "Found cached dataset sem_eval2014_task4_raw (/Users/stevenlimcorn/.cache/huggingface/datasets/Yaxin___sem_eval2014_task4_raw/All/0.0.1/3f2b4b42aa35876b7faba99ae6f73b106955b8c9162c6fc5160fd74497f7790f)\n",
      "No config specified, defaulting to: sem_eval2014_task4_raw/All\n",
      "Found cached dataset sem_eval2014_task4_raw (/Users/stevenlimcorn/.cache/huggingface/datasets/Yaxin___sem_eval2014_task4_raw/All/0.0.1/3f2b4b42aa35876b7faba99ae6f73b106955b8c9162c6fc5160fd74497f7790f)\n",
      "No config specified, defaulting to: sem_eval2014_task4_raw/All\n",
      "Found cached dataset sem_eval2014_task4_raw (/Users/stevenlimcorn/.cache/huggingface/datasets/Yaxin___sem_eval2014_task4_raw/All/0.0.1/3f2b4b42aa35876b7faba99ae6f73b106955b8c9162c6fc5160fd74497f7790f)\n"
     ]
    }
   ],
   "source": [
    "# Load dataset from huggingface\n",
    "from datasets import load_dataset\n",
    "train_dataset = load_dataset(\"Yaxin/SemEval2014Task4Raw\", split='train')\n",
    "validation_dataset = load_dataset(\"Yaxin/SemEval2014Task4Raw\", split='validation')\n",
    "test_dataset = load_dataset(\"Yaxin/SemEval2014Task4Raw\", split='test')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The test dataset doesn't have any polarity but it does have terms and categories."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next will be preprocessing the data. In this sample, most of the text dataset are pretty cleaned. But for the sake of this demo, we will clean it anyways."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsupervised_absa.preprocess import simple_preprocessing\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing strip_spaces: 100%|██████████| 11/11 [00:00<00:00, 67.39it/s]         \n"
     ]
    }
   ],
   "source": [
    "# preprocessing currently only supports pandas.series, list, and numpy\n",
    "train_df = train_dataset.to_pandas()\n",
    "train_df['text'] = simple_preprocessing(train_df['text'])\n",
    "preprocessed_dataset = Dataset.from_pandas(train_df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The processes in the preprocessing pipeline include:\n",
    "1. encode decode\n",
    "2. convert_unicode\n",
    "3. remove_url\n",
    "4. remove_control_characters\n",
    "5. remove_tags\n",
    "6. remove_emoji\n",
    "7. convert_contractions\n",
    "8. remove_numbers\n",
    "9. remove_punctuation\n",
    "10. remove_multiple_spaces\n",
    "11. strip_spaces"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pos Tagging"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of pos tagging to get the potential aspect terms in each sentence. For sentences without aspects being detect will be directed to the sentence embedding extraction section.\n",
    "The pos tagging model used here is from [Flair](https://github.com/flairNLP/flair), a library to perform NLP processing. This project revolves around using this library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-03 14:58:23,731 SequenceTagger predicts: Dictionary with 53 tags: <unk>, O, UH, ,, VBD, PRP, VB, PRP$, NN, RB, ., DT, JJ, VBP, VBG, IN, CD, NNS, NNP, WRB, VBZ, WDT, CC, TO, MD, VBN, WP, :, RP, EX, JJR, FW, XX, HYPH, POS, RBR, JJS, PDT, NNPS, RBS, AFX, WP$, -LRB-, -RRB-, ``, '', LS, $, SYM, ADD\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-03 14:58:23.877 | INFO     | unsupervised_absa.tagger:__init__:32 - Tagger model instantiated with device: cpu\n",
      "2023-04-03 14:58:23.902 | INFO     | unsupervised_absa.tagger:tagging:38 - Extracting pos tags\n",
      " 21%|██        | 1286/6086 [05:20<15:47,  5.07it/s]"
     ]
    }
   ],
   "source": [
    "from unsupervised_absa.tagger import Tagger\n",
    "tagger = Tagger('pos', device='cpu')\n",
    "preprocessed_df = preprocessed_dataset.to_pandas()\n",
    "preprocessed_df['pos_tag'] = tagger.tagging(preprocessed_df['text'], filter_tags=[\"NN\", \"NNS\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performing these pos tags for nouns will give us the candidate terms. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from unsupervised_absa.preprocess import plot_top_k_words\n",
    "pos_tags = preprocessed_df.to_dict('records')\n",
    "list_of_words = []\n",
    "for row in tqdm(pos_tags):\n",
    "    for key, value in row['pos_tag'].items():\n",
    "        pos_tags.append(value)\n",
    "plot_top_k_words(pos_tags, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsupervised_absa.embedding import ExtractEmbedding, ModelType\n",
    "\n",
    "embedding = ExtractEmbedding([ModelType.TRANSFORMER_WORD, ModelType.TRANSFORMER_WORD], ['bert-base-uncased', 'roberta-base'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00,  3.49it/s]\n"
     ]
    }
   ],
   "source": [
    "embedding_data = embedding.extract(df[\"preprocessed\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsupervised_absa.clustering import ClusteringModel\n",
    "from sklearn.cluster import KMeans\n",
    "model = KMeans(n_clusters=2)\n",
    "\n",
    "clustering_model = ClusteringModel(\n",
    "    model=model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-23 15:03:53.939 | INFO     | unsupervised_absa.clustering:fit:46 - Start clustering KMeans(n_clusters=2) with 2 Datapoints.\n",
      "2023-03-23 15:03:53.941 | INFO     | unsupervised_absa.clustering:fit:53 - (2, 1536)\n",
      "/opt/miniconda3/envs/ABSA/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "2023-03-23 15:03:54.430 | INFO     | unsupervised_absa.clustering:fit:55 - Finished clustering.\n"
     ]
    }
   ],
   "source": [
    "clustering_model.fit(embeddings=embedding_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-23 15:03:54.460 | INFO     | unsupervised_absa.clustering:fit:46 - Start clustering Birch(n_clusters=6) with 2 Datapoints.\n",
      "2023-03-23 15:03:54.461 | INFO     | unsupervised_absa.clustering:fit:53 - (2, 1536)\n",
      "/opt/miniconda3/envs/ABSA/lib/python3.10/site-packages/sklearn/cluster/_birch.py:726: ConvergenceWarning: Number of subclusters found (2) by BIRCH is less than (6). Decrease the threshold.\n",
      "  warnings.warn(\n",
      "2023-03-23 15:03:54.508 | INFO     | unsupervised_absa.clustering:fit:55 - Finished clustering.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import Birch\n",
    "model = Birch(n_clusters=6)\n",
    "\n",
    "clustering_model = ClusteringModel(\n",
    "    model=model,\n",
    ")\n",
    "\n",
    "# fit the model on a corpus\n",
    "clustering_model.fit(embedding_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture\n",
    "model = GaussianMixture(n_components=6)\n",
    "\n",
    "clustering_model = ClusteringModel(\n",
    "    model=model,\n",
    ")\n",
    "\n",
    "# fit the model on a corpus\n",
    "clustering_model.fit(embedding_data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNLI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: unsupervised_absa in /opt/miniconda3/envs/ABSA/lib/python3.10/site-packages (0.1.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install unsupervised_absa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No config specified, defaulting to: sem_eval2014_task4_raw/All\n",
      "Found cached dataset sem_eval2014_task4_raw (/Users/stevenlimcorn/.cache/huggingface/datasets/Yaxin___sem_eval2014_task4_raw/All/0.0.1/3f2b4b42aa35876b7faba99ae6f73b106955b8c9162c6fc5160fd74497f7790f)\n"
     ]
    }
   ],
   "source": [
    "from datasets import concatenate_datasets, load_dataset\n",
    "train_dataset = load_dataset(\"Yaxin/SemEval2014Task4Raw\", split='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /Users/stevenlimcorn/.cache/huggingface/datasets/Yaxin___sem_eval2014_task4_raw/All/0.0.1/3f2b4b42aa35876b7faba99ae6f73b106955b8c9162c6fc5160fd74497f7790f/cache-fb60e195c5b3dd16.arrow\n"
     ]
    }
   ],
   "source": [
    "# 1. Convert datasets of semeval (expand the categories to different entries) datasets to df\n",
    "train_dataset.set_format(\"pandas\")\n",
    "train_dataset = train_dataset.filter(lambda x: len(x['aspectTerms']) != 0)[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3509it [00:00, 56160.07it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "new_df = {'text': [], 'sid': [], 'aspectLabel': []}\n",
    "for index, row in tqdm(train_dataset.iterrows()):\n",
    "    text = row['text']\n",
    "    sid = row['sentenceId']\n",
    "    for aspect in row['aspectTerms']:\n",
    "        new_df['aspectLabel'].append(aspect['term'])\n",
    "        new_df['text'].append(text)\n",
    "        new_df['sid'].append(sid)\n",
    "\n",
    "preprocessed_df = pd.DataFrame(new_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "preprocessed_dataset = Dataset.from_pandas(preprocessed_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-large-mnli were not used when initializing DebertaForSequenceClassification: ['config']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from unsupervised_absa.mnli import MnliPipeline\n",
    "model = MnliPipeline('microsoft/deberta-large-mnli')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-03 16:58:39.346 | INFO     | unsupervised_absa.mnli:extract_polarity:146 - Preprocessing dataset with length: 31\n",
      "2023-04-03 16:58:39.354 | INFO     | unsupervised_absa.mnli:extract_polarity:150 - Extracting polarity with model: microsoft/deberta-large-mnli\n",
      "100%|██████████| 31/31 [00:09<00:00,  3.25it/s]\n",
      "2023-04-03 16:58:48.927 | INFO     | unsupervised_absa.mnli:extract_polarity:169 - Postprocessing outputs\n"
     ]
    }
   ],
   "source": [
    "# for batch in [8, 16, 32, 64, 128]:\n",
    "dataset = model.extract_polarity(preprocessed_dataset, 'text', 'aspectLabel', device='mps', batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats = {'a': 5, 'b': 2}\n",
    "max(stats, key=stats.get)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_polarity_term(output: dict):\n",
    "    processed = {}\n",
    "    term = None\n",
    "    for label, score in zip(output[\"labels\"], output[\"scores\"]):\n",
    "        # sample: This example is negative sentiment towards staff.\n",
    "        polarity, term = (\n",
    "            label.replace(\"This example is \", \"\")\n",
    "            .replace(\" sentiment towards\", \"\")\n",
    "            .replace(\".\", \"\")\n",
    "            .split()\n",
    "        )\n",
    "        term = term\n",
    "        processed[polarity] = score\n",
    "    # argmax of previously extracted polarity\n",
    "    processed[\"polarity\"] = max(processed, key=processed.get)\n",
    "    processed[\"term\"] = term\n",
    "    return processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'negative': 0.992158055305481,\n",
       " 'positive': 0.004752019420266151,\n",
       " 'neutral': 0.0030899278353899717,\n",
       " 'polarity': 'negative',\n",
       " 'term': 'staff'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_get_polarity_term({'sequence': 'But the staff was so horrible to us.', 'labels': ['This example is negative sentiment towards staff.', 'This example is positive sentiment towards staff.', 'This example is neutral sentiment towards staff.'], 'scores': [0.992158055305481, 0.004752019420266151, 0.0030899278353899717]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ABSA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b99be7a66b9fc7e2002108bad95f94de35adc958c909aaef2b3bcce7bc0aea9d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
