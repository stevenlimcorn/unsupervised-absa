{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section is used to extract the word embeddings of each nouns that are tagged from pos tagging process. These terms will be clustered to get the specific domain they are in."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Short words removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6055/6055 [00:00<00:00, 3257215.69it/s]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from tqdm.auto import tqdm\n",
    "with open('data/pos tag/semeval_pos_tag_remove_short_words.json') as f:\n",
    "    pos_tags = json.load(f)\n",
    "\n",
    "list_of_words = []\n",
    "for row in tqdm(pos_tags):\n",
    "    for pos_data in row['pos_tag']:\n",
    "        list_of_words.append(pos_data['word'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the list of words, remove all the \n",
    "# convert words to lower case\n",
    "list_of_words = list(set(map(lambda x: x.lower(), list_of_words)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-04-14 16:31:27.084\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36munsupervised_absa.embedding\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m80\u001b[0m - \u001b[1mTagger model instantiated with device: cpu\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from unsupervised_absa.embedding import ExtractEmbedding, ModelType\n",
    "bert_embedding = ExtractEmbedding(ModelType.TRANSFORMER_WORD, 'bert-large-uncased', device=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1393/1393 [03:07<00:00,  7.41it/s]\n"
     ]
    }
   ],
   "source": [
    "bert_embedding_data = bert_embedding.extract(list_of_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.save('data/word embedding data/sem_eval_bert_embedding.npy', bert_embedding_data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bart-Large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-04-14 16:01:49.825\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36munsupervised_absa.embedding\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m80\u001b[0m - \u001b[1mTagger model instantiated with device: cpu\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from unsupervised_absa.embedding import ExtractEmbedding, ModelType\n",
    "bart_embedding = ExtractEmbedding(ModelType.TRANSFORMER_WORD, 'facebook/bart-large', device=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3906/3906 [13:12<00:00,  4.93it/s] \n"
     ]
    }
   ],
   "source": [
    "bart_embedding_data = bart_embedding.extract(list_of_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.save('data/word embedding data/sem_eval_bart_embedding.npy', bart_embedding_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deberta-v3-Large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\anaconda3\\envs\\absa\\lib\\site-packages\\transformers\\convert_slow_tokenizer.py:446: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "\u001b[32m2023-04-13 19:01:21.564\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36munsupervised_absa.embedding\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m80\u001b[0m - \u001b[1mTagger model instantiated with device: cpu\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from unsupervised_absa.embedding import ExtractEmbedding, ModelType\n",
    "deberta_embedding = ExtractEmbedding(ModelType.TRANSFORMER_WORD, 'microsoft/deberta-v3-large')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1393/1393 [13:49<00:00,  1.68it/s]\n"
     ]
    }
   ],
   "source": [
    "deberta_embedding_data = deberta_embedding.extract(list_of_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.save('data/word embedding data/sem_eval_deberta_embedding.npy', deberta_embedding_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-04-13 19:15:14.245\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36munsupervised_absa.embedding\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m80\u001b[0m - \u001b[1mTagger model instantiated with device: cpu\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from unsupervised_absa.embedding import ExtractEmbedding, ModelType\n",
    "glove_embedding = ExtractEmbedding(ModelType.WORD, 'glove')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1393/1393 [00:00<00:00, 5872.58it/s]\n"
     ]
    }
   ],
   "source": [
    "glove_embedding_data = glove_embedding.extract(list_of_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.save('data/word embedding data/sem_eval_glove_embedding.npy', glove_embedding_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-04-13 19:15:27.209\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36munsupervised_absa.embedding\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m80\u001b[0m - \u001b[1mTagger model instantiated with device: cpu\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from unsupervised_absa.embedding import ExtractEmbedding, ModelType\n",
    "fast_text_embedding = ExtractEmbedding(ModelType.WORD, 'en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1393/1393 [00:00<00:00, 5394.54it/s]\n"
     ]
    }
   ],
   "source": [
    "fast_text_embedding_data = fast_text_embedding.extract(list_of_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.save('data/word embedding data/sem_eval_fast_text_embedding.npy', fast_text_embedding_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word2Vec is not in flair, what we can do is convert from gensim to flair"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using Word2Vec pretrained on google news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader\n",
    "model_path = gensim.downloader.load('word2vec-google-news-300', return_path=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model to keyedvector and save it as keyedvector\n",
    "vectors = gensim.models.KeyedVectors.load_word2vec_format(model_path, binary=True)\n",
    "vectors.save('models/word2vec-google.gensim', pickle_protocol=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-04-13 19:16:52.221\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36munsupervised_absa.embedding\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m80\u001b[0m - \u001b[1mTagger model instantiated with device: cpu\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from unsupervised_absa.embedding import ExtractEmbedding, ModelType\n",
    "word2vec_embedding = ExtractEmbedding(ModelType.WORD, 'models/word2vec-google.gensim')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1393/1393 [00:00<00:00, 1637.42it/s]\n"
     ]
    }
   ],
   "source": [
    "word2vec_embedding_data = word2vec_embedding.extract(list_of_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.save('data/word embedding data/sem_eval_word2vec_embedding.npy', word2vec_embedding_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stacked Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stacked embedding of the embedding models [bert, glove, FastText, Word2Vec]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\anaconda3\\envs\\absa\\lib\\site-packages\\transformers\\convert_slow_tokenizer.py:446: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "\u001b[32m2023-04-13 19:25:37.792\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36munsupervised_absa.embedding\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m80\u001b[0m - \u001b[1mTagger model instantiated with device: cpu\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from unsupervised_absa.embedding import ExtractEmbedding, ModelType\n",
    "stacked_embedding = ExtractEmbedding([ModelType.TRANSFORMER_WORD, ModelType.TRANSFORMER_WORD, ModelType.WORD, ModelType.WORD, ModelType.WORD], ['microsoft/deberta-v3-large', 'bert-base-uncased', 'glove', 'en', 'models/word2vec-google.gensim'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1393/1393 [24:47<00:00,  1.07s/it]\n"
     ]
    }
   ],
   "source": [
    "stacked_embedding_data = stacked_embedding.extract(list_of_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.save('data/word embedding data/sem_eval_stacked_embedding.npy', stacked_embedding_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sup-promcse-roberta-large word embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-04-13 19:50:35.373\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36munsupervised_absa.embedding\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m80\u001b[0m - \u001b[1mTagger model instantiated with device: cpu\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from unsupervised_absa.embedding import ExtractEmbedding, ModelType\n",
    "promcse_embedding = ExtractEmbedding(ModelType.TRANSFORMER_DOC, 'YuxinJiang/sup-promcse-roberta-large')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1393/1393 [04:46<00:00,  4.86it/s]\n"
     ]
    }
   ],
   "source": [
    "promcse_embedding_data = promcse_embedding.extract(list_of_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.save('data/word embedding data/promcse_word_embedding.npy', promcse_embedding_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sup-simcse-roberta-large word embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-04-13 19:55:30.908\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36munsupervised_absa.embedding\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m80\u001b[0m - \u001b[1mTagger model instantiated with device: cpu\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from unsupervised_absa.embedding import ExtractEmbedding, ModelType\n",
    "simcse_embedding = ExtractEmbedding(ModelType.TRANSFORMER_DOC, 'princeton-nlp/sup-simcse-roberta-large')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1393/1393 [04:36<00:00,  5.04it/s]\n"
     ]
    }
   ],
   "source": [
    "simcse_embedding_data = simcse_embedding.extract(list_of_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.save('data/word embedding data/simcse_word_embedding.npy', simcse_embedding_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### all-MiniLM-L6-v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-04-13 20:00:09.429\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36munsupervised_absa.embedding\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m80\u001b[0m - \u001b[1mTagger model instantiated with device: cpu\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from unsupervised_absa.embedding import ExtractEmbedding, ModelType\n",
    "all_MiniLM_L6_v2_embedding = ExtractEmbedding(ModelType.TRANSFORMER_DOC, 'sentence-transformers/all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1393/1393 [00:22<00:00, 61.28it/s]\n"
     ]
    }
   ],
   "source": [
    "all_MiniLM_L6_v2_embedding_data = all_MiniLM_L6_v2_embedding.extract(list_of_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.save('data/word embedding data/all_MiniLM_L6_v2_word_embedding.npy', all_MiniLM_L6_v2_embedding_data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Low Frequency (words with count 1) Words Removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6055/6055 [00:00<00:00, 3673732.20it/s]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from tqdm.auto import tqdm\n",
    "with open('data/pos tag/semeval_pos_tag_remove_short_words_and_low_counts.json') as f:\n",
    "    pos_tags = json.load(f)\n",
    "\n",
    "list_of_words = []\n",
    "for row in tqdm(pos_tags):\n",
    "    for pos_data in row['pos_tag']:\n",
    "        list_of_words.append(pos_data['word'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the list of words, remove all the \n",
    "# convert words to lower case\n",
    "list_of_words = list(set(map(lambda x: x.lower(), list_of_words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1393"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list_of_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)okenizer_config.json: 100%|██████████| 28.0/28.0 [00:00<00:00, 16.0kB/s]\n",
      "Downloading (…)lve/main/config.json: 100%|██████████| 571/571 [00:00<00:00, 170kB/s]\n",
      "Downloading (…)solve/main/vocab.txt: 100%|██████████| 232k/232k [00:00<00:00, 595kB/s]\n",
      "Downloading (…)/main/tokenizer.json: 100%|██████████| 466k/466k [00:00<00:00, 2.00MB/s]\n",
      "Downloading pytorch_model.bin: 100%|██████████| 1.34G/1.34G [01:21<00:00, 16.6MB/s]\n",
      "\u001b[32m2023-04-14 16:27:37.585\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36munsupervised_absa.embedding\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m80\u001b[0m - \u001b[1mTagger model instantiated with device: cpu\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from unsupervised_absa.embedding import ExtractEmbedding, ModelType\n",
    "bert_embedding = ExtractEmbedding(ModelType.TRANSFORMER_WORD, 'bert-large-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1393/1393 [03:40<00:00,  6.31it/s]\n"
     ]
    }
   ],
   "source": [
    "bert_embedding_data = bert_embedding.extract(list_of_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.save('data/word embedding data/sem_eval_bert_embedding_word_count.npy', bert_embedding_data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bart-Large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-04-14 16:19:38.267\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36munsupervised_absa.embedding\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m80\u001b[0m - \u001b[1mTagger model instantiated with device: cpu\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from unsupervised_absa.embedding import ExtractEmbedding, ModelType\n",
    "bart_embedding = ExtractEmbedding(ModelType.TRANSFORMER_WORD, 'facebook/bart-large', device=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1393/1393 [03:23<00:00,  6.86it/s]\n"
     ]
    }
   ],
   "source": [
    "bart_embedding_data = bart_embedding.extract(list_of_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.save('data/word embedding data/sem_eval_bart_embedding_word_count.npy', bart_embedding_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deberta-v3-Large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsupervised_absa.embedding import ExtractEmbedding, ModelType\n",
    "deberta_embedding = ExtractEmbedding(ModelType.TRANSFORMER_WORD, 'microsoft/deberta-v3-large')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1393/1393 [24:32<00:00,  1.06s/it]\n"
     ]
    }
   ],
   "source": [
    "deberta_embedding_data = deberta_embedding.extract(list_of_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.save('data/word embedding data/sem_eval_deberta_embedding_word_count.npy', deberta_embedding_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-04-13 20:26:48.862\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36munsupervised_absa.embedding\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m80\u001b[0m - \u001b[1mTagger model instantiated with device: cpu\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from unsupervised_absa.embedding import ExtractEmbedding, ModelType\n",
    "glove_embedding = ExtractEmbedding(ModelType.WORD, 'glove')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1393/1393 [00:00<00:00, 4154.65it/s]\n"
     ]
    }
   ],
   "source": [
    "glove_embedding_data = glove_embedding.extract(list_of_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.save('data/word embedding data/sem_eval_glove_embedding_word_count.npy', glove_embedding_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-04-13 20:27:07.933\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36munsupervised_absa.embedding\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m80\u001b[0m - \u001b[1mTagger model instantiated with device: cpu\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from unsupervised_absa.embedding import ExtractEmbedding, ModelType\n",
    "fast_text_embedding = ExtractEmbedding(ModelType.WORD, 'en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1393/1393 [00:00<00:00, 3428.09it/s]\n"
     ]
    }
   ],
   "source": [
    "fast_text_embedding_data = fast_text_embedding.extract(list_of_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.save('data/word embedding data/sem_eval_fast_text_embedding_word_count.npy', fast_text_embedding_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word2Vec is not in flair, what we can do is convert from gensim to flair"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using Word2Vec pretrained on google news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader\n",
    "model_path = gensim.downloader.load('word2vec-google-news-300', return_path=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model to keyedvector and save it as keyedvector\n",
    "vectors = gensim.models.KeyedVectors.load_word2vec_format(model_path, binary=True)\n",
    "vectors.save('models/word2vec-google.gensim', pickle_protocol=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-04-13 20:29:15.467\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36munsupervised_absa.embedding\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m80\u001b[0m - \u001b[1mTagger model instantiated with device: cpu\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from unsupervised_absa.embedding import ExtractEmbedding, ModelType\n",
    "word2vec_embedding = ExtractEmbedding(ModelType.WORD, 'models/word2vec-google.gensim')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1393/1393 [00:00<00:00, 1543.02it/s]\n"
     ]
    }
   ],
   "source": [
    "word2vec_embedding_data = word2vec_embedding.extract(list_of_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.save('data/word embedding data/sem_eval_word2vec_embedding_word_count.npy', word2vec_embedding_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stacked Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stacked embedding of the embedding models [bert, glove, FastText, Word2Vec]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\anaconda3\\envs\\absa\\lib\\site-packages\\transformers\\convert_slow_tokenizer.py:446: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "\u001b[32m2023-04-13 20:31:47.082\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36munsupervised_absa.embedding\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m80\u001b[0m - \u001b[1mTagger model instantiated with device: cpu\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from unsupervised_absa.embedding import ExtractEmbedding, ModelType\n",
    "stacked_embedding = ExtractEmbedding([ModelType.TRANSFORMER_WORD, ModelType.TRANSFORMER_WORD, ModelType.WORD, ModelType.WORD, ModelType.WORD], ['microsoft/deberta-v3-large', 'bert-base-uncased', 'glove', 'en', 'models/word2vec-google.gensim'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1393/1393 [24:37<00:00,  1.06s/it]\n"
     ]
    }
   ],
   "source": [
    "stacked_embedding_data = stacked_embedding.extract(list_of_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.save('data/word embedding data/sem_eval_stacked_embedding_word_count.npy', stacked_embedding_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sup-promcse-roberta-large word embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-04-13 21:10:01.653\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36munsupervised_absa.embedding\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m80\u001b[0m - \u001b[1mTagger model instantiated with device: cpu\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from unsupervised_absa.embedding import ExtractEmbedding, ModelType\n",
    "promcse_embedding = ExtractEmbedding(ModelType.TRANSFORMER_DOC, 'YuxinJiang/sup-promcse-roberta-large')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1393/1393 [04:10<00:00,  5.57it/s]\n"
     ]
    }
   ],
   "source": [
    "promcse_embedding_data = promcse_embedding.extract(list_of_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.save('data/word embedding data/promcse_word_embedding_word_count.npy', promcse_embedding_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sup-simcse-roberta-large word embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-04-13 21:14:18.024\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36munsupervised_absa.embedding\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m80\u001b[0m - \u001b[1mTagger model instantiated with device: cpu\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from unsupervised_absa.embedding import ExtractEmbedding, ModelType\n",
    "simcse_embedding = ExtractEmbedding(ModelType.TRANSFORMER_DOC, 'princeton-nlp/sup-simcse-roberta-large')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1393/1393 [05:11<00:00,  4.48it/s]\n"
     ]
    }
   ],
   "source": [
    "simcse_embedding_data = simcse_embedding.extract(list_of_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.save('data/word embedding data/simcse_word_embedding_word_count.npy', simcse_embedding_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### all-MiniLM-L6-v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-04-13 21:19:31.160\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36munsupervised_absa.embedding\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m80\u001b[0m - \u001b[1mTagger model instantiated with device: cpu\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from unsupervised_absa.embedding import ExtractEmbedding, ModelType\n",
    "all_MiniLM_L6_v2_embedding = ExtractEmbedding(ModelType.TRANSFORMER_DOC, 'sentence-transformers/all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1393/1393 [00:35<00:00, 39.47it/s]\n"
     ]
    }
   ],
   "source": [
    "all_MiniLM_L6_v2_embedding_data = all_MiniLM_L6_v2_embedding.extract(list_of_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.save('data/word embedding data/all_MiniLM_L6_v2_word_embedding_word_count.npy', all_MiniLM_L6_v2_embedding_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "preprocessed_dataset = load_from_disk('data/preprocessed data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the sentences\n",
    "preprocessed_df = preprocessed_dataset.to_pandas()\n",
    "preprocessed_df = preprocessed_df.drop_duplicates(subset=['text'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### sup-promcse-roberta-large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-04-12 13:28:07.020\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36munsupervised_absa.embedding\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m80\u001b[0m - \u001b[1mTagger model instantiated with device: cpu\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from unsupervised_absa.embedding import ExtractEmbedding, ModelType\n",
    "promcse_embedding = ExtractEmbedding(ModelType.TRANSFORMER_DOC, 'YuxinJiang/sup-promcse-roberta-large')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6055/6055 [17:53<00:00,  5.64it/s]\n"
     ]
    }
   ],
   "source": [
    "promcse_embedding_data = promcse_embedding.extract(preprocessed_df['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.save('data/sentence embedding data/promcse_embedding.npy', promcse_embedding_data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### sup-simcse-roberta-large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-04-12 13:46:04.884\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36munsupervised_absa.embedding\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m80\u001b[0m - \u001b[1mTagger model instantiated with device: cpu\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from unsupervised_absa.embedding import ExtractEmbedding, ModelType\n",
    "simcse_embedding = ExtractEmbedding(ModelType.TRANSFORMER_DOC, 'princeton-nlp/sup-simcse-roberta-large')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6055/6055 [18:21<00:00,  5.50it/s]  \n"
     ]
    }
   ],
   "source": [
    "simcse_embedding_data = simcse_embedding.extract(preprocessed_df['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.save('data/sentence embedding data/simcse_embedding.npy', simcse_embedding_data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### all-MiniLM-L6-v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-04-12 14:04:28.041\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36munsupervised_absa.embedding\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m80\u001b[0m - \u001b[1mTagger model instantiated with device: cpu\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from unsupervised_absa.embedding import ExtractEmbedding, ModelType\n",
    "all_MiniLM_L6_v2_embedding = ExtractEmbedding(ModelType.TRANSFORMER_DOC, 'sentence-transformers/all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6055/6055 [01:10<00:00, 86.18it/s]\n"
     ]
    }
   ],
   "source": [
    "all_MiniLM_L6_v2_embedding_data = all_MiniLM_L6_v2_embedding.extract(preprocessed_df['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.save('data/sentence embedding data/all_MiniLM_L6_v2_embedding.npy', all_MiniLM_L6_v2_embedding_data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stacked Sentence Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stacked embedding of the embedding models [sup-promcse-roberta-large, princeton-nlp/sup-simcse-roberta-large, sentence-transformers/all-MiniLM-L6-v2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-04-12 14:05:47.821\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36munsupervised_absa.embedding\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m80\u001b[0m - \u001b[1mTagger model instantiated with device: cpu\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from unsupervised_absa.embedding import ExtractEmbedding, ModelType\n",
    "stacked_sentence_embedding = ExtractEmbedding([ModelType.TRANSFORMER_DOC, ModelType.TRANSFORMER_DOC, ModelType.TRANSFORMER_DOC], ['YuxinJiang/sup-promcse-roberta-large', 'princeton-nlp/sup-simcse-roberta-large', 'sentence-transformers/all-MiniLM-L6-v2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6055/6055 [37:26<00:00,  2.70it/s]\n"
     ]
    }
   ],
   "source": [
    "stacked_sentence_embedding_data = stacked_sentence_embedding.extract(preprocessed_df['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.save('data/sentence embedding data/stacked_sentence_embedding.npy', stacked_sentence_embedding_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "absa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
